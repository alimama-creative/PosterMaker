import gradio as gr
import json
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import os

import cv2
import torch
import transformers
from diffusers import FlowMatchEulerDiscreteScheduler
import textwrap

from models.adapter_models import LinearAdapterWithLayerNorm
from utils.data_processor import UserInputProcessor
from utils.sd3_utils import *
from utils.utils import post_process
from utils.data_processor import UserInputProcessor
from pipelines.pipeline_sd3 import StableDiffusion3ControlNetPipeline

def check_and_process_texts(texts_str, width, height):
    try:
        if not texts_str:
            raise ValueError("texts_str cannot be None or empty")
            
        texts = json.loads(texts_str)
        
        if not texts or not isinstance(texts, list):
            raise ValueError("Invalid texts format: must be a non-empty list")
            
        if len(texts) > 7:
            raise ValueError("Too many text lines: maximum allowed is 7 lines")
            
        processed_texts = []
        
        for text in texts:
            if not isinstance(text, dict) or 'content' not in text or 'pos' not in text:
                raise ValueError("Invalid text format: each item must be a dict with 'content' and 'pos'")
                
            content = text['content']
            pos = text['pos']
            
            # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
            if len(content) > 16:
                raise ValueError(f"Text too long: '{content}' exceeds 16 characters")
                
            # æ£€æŸ¥å¹¶ä¿®æ­£è¾¹ç•Œå€¼
            x1, y1, x2, y2 = pos
            x1 = max(0, min(x1, width))
            y1 = max(0, min(y1, height))
            x2 = max(0, min(x2, width))
            y2 = max(0, min(y2, height))
            
            # ç¡®ä¿ x1 < x2, y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            
            processed_texts.append({
                "content": content,
                "pos": [x1, y1, x2, y2]
            })
            
        return processed_texts
        
    except json.JSONDecodeError:
        raise ValueError("Invalid JSON format in texts_str")
    except Exception as e:
        raise ValueError(f"Error processing texts: {str(e)}")


class ImageGenerator:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.data_processor = UserInputProcessor()
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œç®¡é“
        self.initialize_models()
        
    def initialize_models(self):
        # è¿™é‡ŒåŠ è½½æ‰€æœ‰å¿…è¦çš„æ¨¡å‹å’Œç»„ä»¶
        args = self.get_default_args()
        
        # load text encoders
        text_encoder_cls_one = import_model_class_from_model_name_or_path(
            args.pretrained_model_name_or_path, args.revision
        )
        text_encoder_cls_two = import_model_class_from_model_name_or_path(
            args.pretrained_model_name_or_path, args.revision, subfolder="text_encoder_2"
        )
        text_encoder_cls_three = import_model_class_from_model_name_or_path(
            args.pretrained_model_name_or_path, args.revision, subfolder="text_encoder_3"
        )
        text_encoder_one, text_encoder_two, text_encoder_three = load_text_encoders(
            args, text_encoder_cls_one, text_encoder_cls_two, text_encoder_cls_three
        ) 
        # Load tokenizers
        tokenizer_one = transformers.CLIPTokenizer.from_pretrained(
            args.pretrained_model_name_or_path,
            subfolder="tokenizer",
            revision=args.revision,
        )
        tokenizer_two = transformers.CLIPTokenizer.from_pretrained(
            args.pretrained_model_name_or_path,
            subfolder="tokenizer_2",
            revision=args.revision,
        )
        tokenizer_three = transformers.T5TokenizerFast.from_pretrained(
            args.pretrained_model_name_or_path,
            subfolder="tokenizer_3",
            revision=args.revision,
        )

        # load vae
        vae = load_vae(args)
        # load sd3
        transformer = load_transfomer(args)
        # load scheduler
        noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(
            args.pretrained_model_name_or_path, subfolder="scheduler"
        )
        # create SceneGenNet
        controlnet_inpaint = load_controlnet(args, transformer, additional_in_channel=1, num_layers=23, scratch=True)
        # create TextRenderNet
        controlnet_text = load_controlnet(args, transformer, additional_in_channel=0, scratch=True)      
        # load adapter
        adapter = LinearAdapterWithLayerNorm(128, 4096)
        
        controlnet_inpaint.load_state_dict(torch.load(args.controlnet_model_name_or_path, map_location='cpu'))
        textrender_net_state_dict = torch.load(args.controlnet_model_name_or_path2, map_location='cpu')
        controlnet_text.load_state_dict(textrender_net_state_dict['controlnet_text'])
        adapter.load_state_dict(textrender_net_state_dict['adapter'])

        # set device and dtype
        weight_dtype = (torch.float16 if torch.cuda.is_available() else torch.float32)
        device = self.device

        # move all models to device
        vae.to(device=device)
        text_encoder_one.to(device=device, dtype=weight_dtype)
        text_encoder_two.to(device=device, dtype=weight_dtype)
        text_encoder_three.to(device=device, dtype=weight_dtype)
        controlnet_inpaint.to(device=device, dtype=weight_dtype)
        controlnet_text.to(device=device, dtype=weight_dtype)
        adapter.to(device=device, dtype=weight_dtype)

        # load pipeline
        pipeline = StableDiffusion3ControlNetPipeline(
            scheduler=FlowMatchEulerDiscreteScheduler.from_config(
                noise_scheduler.config
                ),
            vae=vae,
            transformer=transformer,
            text_encoder=text_encoder_one,
            tokenizer=tokenizer_one,
            text_encoder_2=text_encoder_two,
            tokenizer_2=tokenizer_two,
            text_encoder_3=text_encoder_three,
            tokenizer_3=tokenizer_three,
            controlnet_inpaint=controlnet_inpaint,
            controlnet_text=controlnet_text,
            adapter=adapter,
        )

        self.pipeline = pipeline.to(dtype=weight_dtype, device=device)
        
    def generate(self, main_image, mask_image, texts_str, prompt, seed_generator):
        try:
            # å°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºnumpyæ ¼å¼ï¼ŒRGB
            main_image = np.array(main_image)
            mask = cv2.cvtColor(np.array(mask_image), cv2.COLOR_BGR2GRAY)
            
            # è§£ææ–‡æœ¬å¸ƒå±€
            texts = json.loads(texts_str)
            
            # é¢„å¤„ç†è¾“å…¥æ•°æ®
            input_data = self.data_processor(
                image=main_image,
                mask=mask,
                texts=texts,
                prompt=prompt
            )
            
            # æ‰§è¡Œæ¨ç†
            results = self.pipeline(
                prompt=prompt,
                negative_prompt='deformed, distorted, disfigured, poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, mutated hands and fingers, disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputation, NSFW',
                height=1024,
                width=1024,
                control_image=[input_data['cond_image_inpaint'], input_data['controlnet_im']],
                control_mask=input_data['control_mask'],
                text_embeds=input_data['text_embeds'],
                num_inference_steps=(28 if torch.cuda.is_available() else 1),
                generator=seed_generator,
                controlnet_conditioning_scale=1.0,
                guidance_scale=5.0,
                num_images_per_prompt=1,
            ).images
            
            # åå¤„ç†ï¼Œæ ¹æ®im_h, im_wä»relä¸­è£å‰ª[0, 0, im_w, im_h]åŒºåŸŸ
            rel = post_process(results[0], input_data['target_size'])
            
            # è¿”å›ç”Ÿæˆçš„å›¾åƒ
            return rel
            
        except Exception as e:
            return f"Error in image generation: {str(e)}"
        
    def get_default_args(self):
        # è¿”å›é»˜è®¤å‚æ•°é…ç½®
        class Args:
            def __init__(self):
                self.pretrained_model_name_or_path = './checkpoints/stable-diffusion-3-medium-diffusers/'
                self.controlnet_model_name_or_path='./checkpoints/ours_weights/scenegen_net-1m-0415.pth'
                self.controlnet_model_name_or_path2='./checkpoints/ours_weights/textrender_net-1m-0415.pth'
                self.revision = None
        return Args()


def visualize_layout(main_image, mask_image, texts_str, prompt,
                     font_path: str = "./assets/fonts/AlibabaPuHuiTi-3-55-Regular.ttf",
                     margin_ratio: float = 0.92):
    """
    æ¸²æŸ“å¸¦æœ‰æ–‡å­— bbox çš„å¸ƒå±€ç¤ºæ„å›¾  
    - texts_str -> [{'pos': (x1,y1,x2,y2), 'content': "..."}] ç”±å¤–éƒ¨ `check_and_process_texts` è§£æ
    - æ–‡æœ¬å¤§å°ã€ä½ç½®ä¼šæ ¹æ®æ¡†å¤§å°è‡ªåŠ¨è°ƒæ•´å¹¶ä¿æŒå±…ä¸­
    """
    try:
        # -------- 1. è·å–ç”»å¸ƒå°ºå¯¸ -------- #
        if main_image is not None:
            height, width = main_image.shape[:2]
        elif mask_image is not None:
            height, width = mask_image.shape[:2]
        else:
            width = height = 1024

        # -------- 2. åº•å›¾ä¸ mask åˆæˆ -------- #
        canvas = Image.new("RGBA", (width, height), "white")

        if main_image is not None:
            pil_main = Image.fromarray(main_image).convert("RGBA")
        else:
            pil_main = Image.new("RGBA", (width, height), (0, 0, 0, 0))

        if mask_image is not None:
            gray = cv2.cvtColor(mask_image, cv2.COLOR_BGR2GRAY)
            alpha = Image.fromarray(np.where(gray > 127, 255, 0).astype(np.uint8))
            pil_main.putalpha(alpha)

        canvas.alpha_composite(pil_main)

        # -------- 3. å·¥å…·å‡½æ•° -------- #
        def get_font(size):
            """ä¼˜é›…åœ°é™çº§åˆ°é»˜è®¤å­—ä½“"""
            try:
                return ImageFont.truetype(font_path, size)
            except OSError:
                return ImageFont.load_default()

        def optimal_font_size(text, box_w, box_h, draw):
            """äºŒåˆ†æœç´¢å¯å®¹çº³æœ€å¤§å­—å·ï¼ˆæ— éœ€ä¼  font_pathï¼Œå› ä¸º get_font è‡ªå¸¦å›é€€ï¼‰"""
            low, high, best = 1, box_h, 1
            while low <= high:
                mid = (low + high) // 2
                font = get_font(mid)
                bbox = draw.textbbox((0, 0), text, font=font)
                txt_w = bbox[2] - bbox[0]
                txt_h = bbox[3] - bbox[1]
                if txt_w <= box_w * margin_ratio and txt_h <= box_h * margin_ratio:
                    best = mid
                    low = mid + 1
                else:
                    high = mid - 1
            return best

        def wrap_text(text, font, box_w, draw):
            """æ ¹æ®å®½åº¦è‡ªåŠ¨æ¢è¡Œï¼›è¿”å› lines åˆ—è¡¨"""
            words = text.split()  # å…¼å®¹è‹±æ–‡ç©ºæ ¼ï¼Œä¸­æ–‡ç©ºæ ¼åŒç†
            if len(words) == 1:
                # çº¯ä¸­æ–‡æˆ–æ²¡æœ‰ç©ºæ ¼æ—¶ï¼šæŒ‰å­—ç¬¦ç²—æš´æˆªæ–­
                wrapped = textwrap.wrap(text, width=len(text))
            else:
                wrapped = textwrap.wrap(text, width=len(words))
            # å°è¯•ä¸æ–­æ‰©è¡Œï¼Œç›´åˆ°æ‰€æœ‰è¡Œå®½éƒ½ <= box_w*margin_ratio
            while True:
                line_too_long = False
                for i, line in enumerate(wrapped):
                    if draw.textlength(line, font=font) > box_w * margin_ratio:
                        # æŠŠè¯¥è¡Œå†æ‹†ä¸€åŠ
                        midpoint = max(1, len(line) // 2)
                        wrapped[i:i+1] = [line[:midpoint], line[midpoint:]]
                        line_too_long = True
                        break
                if not line_too_long:
                    break
            return wrapped

        draw = ImageDraw.Draw(canvas)

        # -------- 4. æ¸²æŸ“æ¯ä¸ªæ–‡æœ¬æ¡† -------- #
        texts = check_and_process_texts(texts_str, width, height)

        for item in texts:
            (x1, y1, x2, y2) = item["pos"]
            content = item["content"]

            box_w, box_h = x2 - x1, y2 - y1

            # 4.1 è®¡ç®—æœ€ä½³å­—å·
            size = optimal_font_size(content, box_w, box_h, draw)
            font = get_font(size)

            # 4.2 å¦‚æœå•è¡Œä»è¶…å®½åˆ™è‡ªåŠ¨æ¢è¡Œå¹¶è°ƒæ•´å­—å·
            lines = wrap_text(content, font, box_w, draw)
            # è‹¥æ¢è¡Œåæ€»é«˜åº¦è¶…æ¡†ï¼Œå†ç¼©å°å­—å·
            line_height = size * 1.2
            while line_height * len(lines) > box_h * margin_ratio and size > 1:
                size -= 1
                font = get_font(size)
                lines = wrap_text(content, font, box_w, draw)
                line_height = size * 1.2

            # 4.3 è®¡ç®—æ•´ä½“æ–‡æœ¬å—å°ºå¯¸
            txt_h = line_height * len(lines)
            txt_w = max(draw.textlength(line, font=font) for line in lines)

            # å·¦ä¸Šè§’åæ ‡ï¼ˆå±…ä¸­ï¼‰
            start_x = round(x1 + (box_w - txt_w) / 2)
            start_y = round(y1 + (box_h - txt_h) / 2)

            # 4.4 ç»˜åˆ¶ bbox
            draw.rectangle([x1, y1, x2, y2], outline="red", width=2)

            # 4.5 ç»˜åˆ¶æ–‡å­—
            for idx, line in enumerate(lines):
                draw.text((start_x, start_y + idx * line_height),
                          line, fill="blue", font=font, align="center")

        return canvas.convert("RGB")  # ä¸æ—§æ¥å£ä¿æŒä¸€è‡´

    except Exception as e:
        # è¿”å› Image ä»¥å¤–çš„å¯¹è±¡å¯èƒ½ä¼šç ´åè°ƒç”¨é“¾ï¼Œç›´æ¥ raise æ›´å¥½
        return RuntimeError(f"visualize_layout failed: {e}")


# ä¿®æ”¹generate_imageå‡½æ•°æ¥ä½¿ç”¨ImageGenerator
generator = ImageGenerator()

def generate_image(main_image, mask_image, texts_str, prompt, seed):
    if main_image is None:
        return "Error: Main image is required"

    try:
        # å¤„ç†main_imageçš„æ ¼å¼
        if isinstance(main_image, np.ndarray):
            # å¤„ç†numpy arrayæ ¼å¼
            if main_image.ndim == 3:
                if main_image.shape[2] == 4:  # RGBAæ ¼å¼
                    rgb_array = main_image[..., :3]
                    alpha_channel = main_image[..., 3]
                    main_image = Image.fromarray(rgb_array)
                    # å¦‚æœæ²¡æœ‰æä¾›maskï¼Œä½¿ç”¨alphaé€šé“ä½œä¸ºmask
                    if mask_image is None:
                        mask_array = (alpha_channel > 128).astype(np.uint8) * 255
                        mask_image = Image.fromarray(mask_array)
                elif main_image.shape[2] == 3:  # RGBæ ¼å¼
                    main_image = Image.fromarray(main_image)
                    if mask_image is None:
                        return "Error: When using RGB image, a mask image must be provided"
                else:
                    return "Error: Invalid number of channels in main image"
            else:
                return "Error: Invalid dimensions for main image"
        elif isinstance(main_image, Image.Image):
            # å¤„ç†PIL.Imageæ ¼å¼
            if main_image.mode == 'RGBA':
                rgb_image = main_image.convert('RGB')
                alpha_channel = main_image.split()[3]
                # å¦‚æœæ²¡æœ‰æä¾›maskï¼Œä½¿ç”¨alphaé€šé“ä½œä¸ºmask
                if mask_image is None:
                    mask_image = alpha_channel.point(lambda x: 255 if x > 128 else 0)
                main_image = rgb_image
            elif main_image.mode != 'RGB' and mask_image is None:
                return "Error: When using RGB image, a mask image must be provided"
        else:
            return "Error: Main image must be numpy array or PIL.Image format"

        # ç¡®ä¿main_imageæ˜¯RGBæ¨¡å¼
        if isinstance(main_image, Image.Image) and main_image.mode != 'RGB':
            main_image = main_image.convert('RGB')

        # å¤„ç†mask_imageçš„æ ¼å¼
        if mask_image is not None:
            if isinstance(mask_image, np.ndarray):
                mask_image = Image.fromarray(mask_image)
            elif not isinstance(mask_image, Image.Image):
                return "Error: Mask image must be numpy array or PIL.Image format"

        # ä½¿ç”¨è®¾å®šçš„seed
        seed_generator = torch.Generator(device=generator.device).manual_seed(int(seed))
        # ä½¿ç”¨ImageGeneratorç”Ÿæˆå›¾åƒ
        generated_image = generator.generate(main_image, mask_image, texts_str, prompt, seed_generator)
        return generated_image
    except Exception as e:
        return f"Error: {str(e)}"

# For debugging
# def generate_image(main_image, mask_image, texts_str, prompt, seed):
#     try:
#         # è¿™é‡Œæ˜¯ç”Ÿæˆå›¾åƒçš„é€»è¾‘
#         # ç°åœ¨åªè¿”å›ä¸€ä¸ªå ä½å›¾åƒ
#         if main_image is None:
#             return "Error: Main image is required"
            
#         generated_image = Image.fromarray(main_image)  # ä¸´æ—¶ä½¿ç”¨è¾“å…¥å›¾åƒä½œä¸ºè¾“å‡º
#         return generated_image
    
#     except Exception as e:
#         return f"Error: {str(e)}"


# æ¸…é™¤æ‰€æœ‰è¾“å…¥å’Œè¾“å‡ºçš„å‡½æ•°
def clear_all():
    return [None, None, "", "", 42, None, None]

# ä¿®æ”¹Gradioç•Œé¢éƒ¨åˆ†
with gr.Blocks() as iface:
    gr.Markdown("""
    # ğŸ¨ [CVPR2025] PosterMaker: Towards High-Quality Product Poster Generation with Accurate Text Rendering

    ## æ–‡å­—æµ·æŠ¥å›¾åƒç”Ÿæˆ | A text poster image generation
                
    ## **ä½œè€… | Authors:** Yifan Gao\*, Zihang Lin\*, Chuanbin Liu, Min Zhou, Tiezheng Ge, Bo Zheng, Hongtao Xie
                                
    <div style="display: flex; gap: 10px; justify-content: left;">
        <a href="https://github.com/eafn/PosterMaker"><img src="https://img.shields.io/badge/GitHub-Repository-blue?logo=github" alt="GitHub"></a>
        <a href="https://arxiv.org/abs/2504.06632"><img src="https://img.shields.io/badge/Paper-arXiv-red?logo=arxiv" alt="Paper"></a>
    </div>    
    """)
    gr.Markdown("""
        ---
        ## ğŸ“ æ–‡æœ¬å¸ƒå±€æ ¼å¼ | Text Layout Format
        """)

    with gr.Row():
        with gr.Column():
            gr.Markdown("""
            ### æ–‡æœ¬JSONæ ¼å¼è¦æ±‚ | Text JSON Format Requirements:
            ```json
            [
                {"content": "ç¬¬ä¸€è¡Œæ–‡æœ¬", "pos": [x1, y1, x2, y2]},
                {"content": "ç¬¬äºŒè¡Œæ–‡æœ¬", "pos": [x1, y1, x2, y2]}
            ]
            ```
            """)
        
        with gr.Column():
            gr.Markdown("""
            ### æ–‡æœ¬é™åˆ¶ | Text Limitations:
            - æœ€å¤š7è¡Œæ–‡æœ¬ | Maximum 7 lines of text
            - æ¯è¡Œâ‰¤16ä¸ªå­—ç¬¦ | â‰¤16 characters per line
            - åæ ‡ä¸è¶…è¿‡å›¾åƒè¾¹ç•Œ | Coordinates within image boundaries
            """)

    # ç¬¬ä¸€æ’ï¼šæ–‡æœ¬è¾“å…¥æ¡†å’Œseedè®¾ç½®
    with gr.Row():
        texts_input = gr.Textbox(
            label="Input JSON text layout", 
            lines=6,
            placeholder="Enter the layout JSON here...",
            scale=1,
        )
        prompt_input = gr.Textbox(
            label="Prompt", 
            lines=6,
            placeholder="Enter the generation prompt here...",
            scale=1,
        )
        seed_input = gr.Slider(
            label="Seed",
            minimum=0,
            maximum=10000,
            step=1,  # æ­¥é•¿ä¸º1ï¼Œç¡®ä¿æ˜¯æ•´æ•°
            value=42,
            scale=0.3,
        )
    
    gr.Markdown("""
        ---
        ## ğŸ“· å›¾åƒä¸Šä¼ è§„åˆ™ | Image Upload Rules:
        """)

    with gr.Row():
        with gr.Column():
            gr.Markdown("""
            ### ä¸»å›¾åƒ(å¿…éœ€) | Subject Image (Required):
            - æ”¯æŒRGBæ ¼å¼ | Supports RGB format

            ### è’™ç‰ˆå›¾åƒ(å¿…éœ€) | Mask Image (Required):
            - RGBå›¾åƒå¿…é¡»ä¸Šä¼ é¢å¤–çš„è’™ç‰ˆå›¾åƒ | RGB image must have a separate mask image
            """)
        
        with gr.Column():
            gr.Markdown("""
            ### è’™ç‰ˆè§„åˆ™ | Mask Rules:
            - ç™½è‰²åŒºåŸŸï¼šä¿ç•™çš„éƒ¨åˆ† | White areas: areas to keep
            - é»‘è‰²åŒºåŸŸï¼šç”Ÿæˆçš„éƒ¨åˆ† | Black areas: areas to generate
            """)

    # ç¬¬äºŒæ’ï¼šå›¾åƒè¾“å…¥
    with gr.Row():
        with gr.Column(scale=1):
            main_image_input = gr.Image(
                label="Upload Subject Image", 
                height=400,
            )
        with gr.Column(scale=1):
            mask_image_input = gr.Image(
                label="Upload Mask Image", 
                height=400,
            )
    
    # æé†’ä¿¡æ¯
    gr.Markdown("""
        ---
        ## âš ï¸ é‡è¦æç¤º | Important Notes:
        """)
    with gr.Row():
        with gr.Column():
            gr.Markdown("""
            ### é¢„è§ˆæ­¥éª¤ | Preview Steps:
            - è¯·å…ˆä½¿ç”¨"Visualize Layout"æŒ‰é’®é¢„è§ˆæ–‡æœ¬å¸ƒå±€ | Please use "Visualize Layout" button first to preview text layout
            - ç¡®è®¤å¸ƒå±€æ— è¯¯åå†ç‚¹å‡»"Generate Image"ç”Ÿæˆå›¾åƒ | Click "Generate Image" after confirming the layout is correct
            """)
        
        with gr.Column():
            gr.Markdown("""
            ### ç­‰å¾…è¯´æ˜ | Wait Time:
            - å›¾åƒç”Ÿæˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾… | Image generation may take some time, please be patient
            """)
        
    # ç¬¬ä¸‰æ’ï¼šæŒ‰é’®
    with gr.Row():
        visualize_btn = gr.Button("Visualize Layout")
        generate_btn = gr.Button("Generate Image")
        clear_btn = gr.Button("Clear All")
    
    # ç¬¬å››æ’ï¼šè¾“å‡ºå›¾åƒ
    with gr.Row():
        with gr.Column(scale=1):
            layout_output = gr.Image(
                label="Layout Visualization", 
                height=400,
            )
        with gr.Column(scale=1):
            generated_output = gr.Image(
                label="Generated Image", 
                height=400,
            )

    gr.Markdown("""
        ---
        ## ç¤ºä¾‹ | Examples:
        """)
    # è®¾ç½®ç¤ºä¾‹
    examples = [
        [
            json.dumps([
                {"content": "æŠ¤è‚¤ç¾é¢œè´µå¦‡ä¹³", "pos": [69, 104, 681, 185]},
                {"content": "99.9%çº¯åº¦ç»è‰²å› ", "pos": [165, 226, 585, 272]},
                {"content": "æŒä¹…ä¿å¹´è½»", "pos": [266, 302, 483, 347]}
            ], ensure_ascii=False),
            "The subject rests on a smooth, dark wooden table, surrounded by a few scattered leaves and delicate flowers,with a serene garden scene complete with blooming flowers and lush greenery in the background.",
            './images/rgba_images/571507774301.png',
            './images/subject_masks/571507774301.png',
            42
        ],
        [
            json.dumps([
                {"content": "å¢å¼ºå…ç–«åŠ›", "pos": [38, 38, 471, 127]},
                {"content": "å¹¼å„¿å¥¶ç²‰", "pos": [38, 143, 356, 224]},
                {"content": "æ˜“äºå†²è°ƒ", "pos": [67, 259, 219, 296]}
            ], ensure_ascii=False),
            "The golden can of milk powder rests on a smooth, dark wooden table, surrounded by a few scattered leaves and delicate flowers, with a serene garden scene complete with blooming flowers and lush greenery in the background.",
            './images/rgba_images/652158680541.png',
            './images/subject_masks/652158680541.png',
            42
        ],
        [
            json.dumps([
                {"content": "CABæ’ä¹…æ°”å«", "pos": [85, 101, 720, 192]},
                {"content": "æŒä¹…ä¸è„±å¦†", "pos": [294, 226, 511, 271]}
            ], ensure_ascii=False),
            "A subject sits elegantly on smooth, light beige fabric, surrounded by a backdrop of similarly draped material that offers a silky appearance. To the left, a delicate white flower injects a subtle natural element into the composition. The overall environment is clean, bright, and minimalistic, exuding a sense of sophistication and simplicity that highlights the subject beautifully.",
            './images/rgba_images/809702153676.png',
            './images/subject_masks/809702153676.png',
            888
        ],
            [
        json.dumps([
            {"content": "åŸåˆ›æ–°æ¬¾", "pos": [135, 60, 686, 199]},
            {"content": "å¡é€šæ¬¾æ‰‹æœºå£³", "pos": [246, 236, 575, 299]}
        ], ensure_ascii=False),
        "The poster features a vibrant yellow background adorned with playful cartoons, including rainbows, clouds, and stars. Characters perform activities like carrying bags and holding hearts, adding a dynamic feel. Comic-style text amplifies the cheerful vibe. The solid yellow backdrop ensures the product stands out. The poster uses eye-catching fonts and text, offering clear visuals that blend harmonious design with sophistication.",
        './images/rgba_images/749870344644.png',
        './images/subject_masks/749870344644.png',
        1000
        ]
    ]
    gr.Examples(
        examples=examples,
        inputs=[texts_input, prompt_input, main_image_input, mask_image_input, seed_input]
    )

    # è®¾ç½®æŒ‰é’®äº‹ä»¶
    visualize_btn.click(
        fn=visualize_layout,
        inputs=[main_image_input, mask_image_input, texts_input, prompt_input],
        outputs=layout_output
    )
    
    generate_btn.click(
        fn=generate_image,
        inputs=[main_image_input, mask_image_input, texts_input, prompt_input, seed_input],
        outputs=generated_output
    )
    
    # æ¸…é™¤æŒ‰é’®äº‹ä»¶
    clear_btn.click(
        fn=clear_all,
        inputs=[],
        outputs=[main_image_input, mask_image_input, texts_input, prompt_input, 
                seed_input, layout_output, generated_output]
    )


# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    iface.launch(server_name="0.0.0.0",server_port=7861)

